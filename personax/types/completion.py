from __future__ import annotations

import typing as t

from personax.types.usage import Usage


class CompletionMessage(t.NamedTuple):
    """LLM completion message content.

    Contains the generated text response and associated metadata from
    the language model.

    Attributes:
        reason: Optional reasoning or explanation for the response.
            Provider-specific field, may be None.
        content: The generated text content. May be None if the response
            was refused or contained only tool calls.
        refusal: Optional refusal message if the model declined to respond
            (e.g., for safety or policy reasons).
    """

    reason: str | None = None
    content: str | None = None
    refusal: str | None = None


class Completion(t.NamedTuple):
    """Complete LLM response with metadata.

    Represents the final output from a non-streaming completion request,
    including the generated message, usage statistics, and completion metadata.

    Attributes:
        id: Unique identifier for this completion. Either provided via
            chatcmpl_id parameter or generated by the provider.
        message: The completion message with generated content.
        finish_reason: Reason why the completion ended:
            - "stop": Natural completion point
            - "length": Hit max_completion_tokens limit
            - "content_filter": Filtered for policy/safety reasons
        created: Unix timestamp when the completion was created.
        model: Model identifier (metadata field, set via model parameter).
        usage: Token usage statistics, if provided by the model.
    """

    id: str
    message: CompletionMessage
    finish_reason: t.Literal["stop", "length", "content_filter"]
    created: int
    model: str
    usage: Usage | None = None
